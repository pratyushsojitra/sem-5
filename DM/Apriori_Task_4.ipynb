{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1422b500",
   "metadata": {},
   "source": [
    "# Apriori Algorithm Implementation Assignment\n",
    "\n",
    "### Objective:\n",
    "You will implement the **Apriori algorithm** from scratch (i.e., without using any libraries like `mlxtend`) to find frequent itemsets and generate association rules.\n",
    "\n",
    "### Dataset:\n",
    "Use the [Online Retail Dataset](https://www.kaggle.com/datasets/vijayuv/onlineretail) from Kaggle. You can filter it for a specific country (e.g., `United Kingdom`) and time range to reduce size if needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85128a0",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "- Load the dataset\n",
    "- Remove rows with missing values\n",
    "- Filter out rows where `Quantity <= 0`\n",
    "- Convert Data into Basket Format\n",
    "\n",
    "ðŸ‘‰ **Implement code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd66a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Preprocess as per the instructions above | We have already done in TASK 2\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Online Retail.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Preprocess the dataset\n",
    "\n",
    "df.dropna(subset=['CustomerID'], inplace=True)  # Drop rows with missing Customer\n",
    "\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])  # Convert InvoiceDate to datetime\n",
    "\n",
    "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']  # TotalPrice column\n",
    "\n",
    "df = df[df['Quantity'] > 0]  # Filter out negative quantities\n",
    "\n",
    "df = df[df['UnitPrice'] > 0]  # Filter out negative unit prices\n",
    "\n",
    "df = df[df['Country'] == 'United Kingdom']  # Filter for UK customers\n",
    "\n",
    "df = df[df['Description'].notna()]  # Filter out rows with missing descriptions\n",
    "\n",
    "df['Description'] = df['Description'].str.strip()  # Strip whitespace from descriptions\n",
    "\n",
    "df = df[df['Description'] != '']  # Remove empty descriptions\n",
    "\n",
    "df['InvoiceNo'] = df['InvoiceNo'].astype(str)  # Ensure InvoiceNo is string type\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(subset=['InvoiceNo', 'StockCode', 'Description', 'Quantity','InvoiceDate', 'UnitPrice', 'CustomerID'], inplace=True)\n",
    "\n",
    "# for Basket\n",
    "basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
    "basket = basket.applymap(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37baf6",
   "metadata": {},
   "source": [
    "## Step 2: Implement Apriori Algorithm\n",
    "Step-by-Step Procedure:\n",
    "1. Generate Frequent 1-Itemsets\n",
    "Count the frequency (support) of each individual item in the dataset.\n",
    "Keep only those with support â‰¥ min_support.\n",
    "â†’ Result is L1 (frequent 1-itemsets)\n",
    "2. Iterative Candidate Generation (k = 2 to n)\n",
    "While L(k-1) is not empty:\n",
    "a. Candidate Generation\n",
    "\n",
    "Generate candidate itemsets Ck of size k from L(k-1) using the Apriori property:\n",
    "Any (k-itemset) is only frequent if all of its (kâˆ’1)-subsets are frequent.\n",
    "b. Prune Candidates\n",
    "Eliminate candidates that have any (kâˆ’1)-subset not in L(k-1).\n",
    "c. Count Support\n",
    "For each transaction, count how many times each candidate in Ck appears.\n",
    "d. Generate Frequent Itemsets\n",
    "Form Lk by keeping candidates from Ck that meet the min_support.\n",
    "Repeat until Lk becomes empty.\n",
    "Implement the following functions:\n",
    "1. `get_frequent_itemsets(transactions, min_support)` - Returns frequent itemsets and their support\n",
    "2. `generate_candidates(prev_frequent_itemsets, k)` - Generates candidate itemsets of length `k`\n",
    "3. `calculate_support(transactions, candidates)` - Calculates the support count for each candidate\n",
    "\n",
    "**Write reusable functions** for each part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8f9310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def get_frequent_itemsets(transactions, min_support):\n",
    "    frequent_itemsets = []\n",
    "    k = 1\n",
    "\n",
    "    if 0 < min_support < 1:\n",
    "        min_support = min_support * len(transactions)\n",
    "\n",
    "    candidates = [(item,) for item in transactions.columns]\n",
    "\n",
    "    while candidates:\n",
    "        if k == 1:\n",
    "            candidate_itemsets = candidates\n",
    "        else:\n",
    "            candidate_itemsets = generate_candidates(candidates, k)\n",
    "        \n",
    "        support_counts = calculate_support(transactions, candidate_itemsets)\n",
    "        \n",
    "        frequent_itemsets_k = {itemset: count for itemset, count in support_counts.items() if count >= min_support}\n",
    "        \n",
    "        if not frequent_itemsets_k:\n",
    "            break\n",
    "\n",
    "        frequent_itemsets.extend(frequent_itemsets_k.items())\n",
    "        candidates = list(frequent_itemsets_k.keys())\n",
    "        k += 1\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "def generate_candidates(prev_frequent_itemsets, k):\n",
    "    candidates = []\n",
    "    for i in range(len(prev_frequent_itemsets)):\n",
    "        for j in range(i + 1, len(prev_frequent_itemsets)):\n",
    "            candidate = tuple(sorted(set(prev_frequent_itemsets[i]) | set(prev_frequent_itemsets[j])))\n",
    "            if len(candidate) == k and candidate not in candidates:\n",
    "                candidates.append(candidate)\n",
    "    return candidates\n",
    "\n",
    "def calculate_support(transactions, candidates):\n",
    "    support_counts = {}\n",
    "    for candidate in candidates:\n",
    "        count = transactions[list(candidate)].all(axis=1).sum()\n",
    "        support_counts[candidate] = count\n",
    "    return support_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c0fe",
   "metadata": {},
   "source": [
    "## Step 3: Generate Association Rules\n",
    "\n",
    "- Use frequent itemsets to generate association rules\n",
    "- For each rule `A => B`, calculate:\n",
    "  - **Support**\n",
    "  - **Confidence**\n",
    "- Only return rules that meet a minimum confidence threshold (e.g., 0.5)\n",
    "\n",
    "ðŸ‘‰ **Implement rule generation function below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97bc236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
    "    rules = []\n",
    "    num_transactions = len(transactions)\n",
    "    \n",
    "    # Convert one-hot encoded DataFrame to list of sets (item names)\n",
    "    transaction_sets = []\n",
    "    for idx, row in transactions.iterrows():\n",
    "        items = set(row.index[row == 1].tolist())\n",
    "        transaction_sets.append(items)\n",
    "\n",
    "    for itemset, support_count in frequent_itemsets:\n",
    "        itemset = set(itemset)\n",
    "        \n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "        \n",
    "        subsets = [set(sub) for i in range(1, len(itemset)) for sub in combinations(itemset, i)]\n",
    "        \n",
    "        for subset in subsets:\n",
    "            subset_support_count = sum(1 for transaction in transaction_sets if subset.issubset(transaction))\n",
    "            confidence = support_count / subset_support_count if subset_support_count > 0 else 0\n",
    "            \n",
    "            if confidence >= min_confidence:\n",
    "                rules.append((subset, itemset - subset, confidence))\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf26889",
   "metadata": {},
   "source": [
    "## Step 4: Output and Visualize\n",
    "\n",
    "- Print top 10 frequent itemsets\n",
    "- Print top 10 association rules (by confidence or lift)\n",
    "\n",
    "ðŸ‘‰ **Output results below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3443a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the final results\n",
    "# Optional: Add visualizations\n",
    "min_support = 0.02     # absolute count threshold\n",
    "min_confidence = 0.6  # relative threshold\n",
    "frequent_itemsets = get_frequent_itemsets(basket, min_support)\n",
    "rules = generate_rules(frequent_itemsets, basket, min_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bde9c32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Frequent Itemsets:\n",
      "Itemset: ('3 STRIPEY MICE FELTCRAFT',), Support: 373\n",
      "Itemset: ('6 RIBBONS RUSTIC CHARM',), Support: 628\n",
      "Itemset: ('60 CAKE CASES VINTAGE CHRISTMAS',), Support: 429\n",
      "Itemset: ('60 TEATIME FAIRY CAKE CASES',), Support: 587\n",
      "Itemset: ('72 SWEETHEART FAIRY CAKE CASES',), Support: 444\n",
      "Itemset: ('ALARM CLOCK BAKELIKE GREEN',), Support: 690\n",
      "Itemset: ('ALARM CLOCK BAKELIKE IVORY',), Support: 424\n",
      "Itemset: ('ALARM CLOCK BAKELIKE PINK',), Support: 491\n",
      "Itemset: ('ALARM CLOCK BAKELIKE RED',), Support: 758\n",
      "Itemset: ('ANTIQUE SILVER T-LIGHT GLASS',), Support: 522\n",
      "\n",
      "Top 10 Association Rules:\n",
      "Rule: {'ALARM CLOCK BAKELIKE GREEN'} -> {'ALARM CLOCK BAKELIKE RED'}, Confidence: 0.66\n",
      "Rule: {'GARDENERS KNEELING PAD CUP OF TEA'} -> {'GARDENERS KNEELING PAD KEEP CALM'}, Confidence: 0.73\n",
      "Rule: {'GARDENERS KNEELING PAD KEEP CALM'} -> {'GARDENERS KNEELING PAD CUP OF TEA'}, Confidence: 0.62\n",
      "Rule: {'PINK REGENCY TEACUP AND SAUCER'} -> {'GREEN REGENCY TEACUP AND SAUCER'}, Confidence: 0.82\n",
      "Rule: {'GREEN REGENCY TEACUP AND SAUCER'} -> {'PINK REGENCY TEACUP AND SAUCER'}, Confidence: 0.66\n",
      "Rule: {'ROSES REGENCY TEACUP AND SAUCER'} -> {'GREEN REGENCY TEACUP AND SAUCER'}, Confidence: 0.70\n",
      "Rule: {'GREEN REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER'}, Confidence: 0.78\n",
      "Rule: {'JUMBO BAG PINK POLKADOT'} -> {'JUMBO BAG RED RETROSPOT'}, Confidence: 0.62\n",
      "Rule: {'JUMBO BAG STRAWBERRY'} -> {'JUMBO BAG RED RETROSPOT'}, Confidence: 0.64\n",
      "Rule: {'PAPER CHAIN KIT VINTAGE CHRISTMAS'} -> {\"PAPER CHAIN KIT 50'S CHRISTMAS\"}, Confidence: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Print top 10 frequent itemsets\n",
    "# Print top 10 association rules (by confidence or lift)\n",
    "print(\"Top 10 Frequent Itemsets:\")\n",
    "for itemset, count in frequent_itemsets[:10]:\n",
    "    print(f\"Itemset: {itemset}, Support: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Association Rules:\")\n",
    "for antecedent, consequent, confidence in rules[:10]:\n",
    "    print(f\"Rule: {set(antecedent)} -> {set(consequent)}, Confidence: {confidence:.2f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da4796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
